@inproceedings{RefWorks:1,
	author={D. Bell and L. Gana},
	year={2012},
	title={Algorithmic Trading Systems: A Multifaceted View of Adoption},
	booktitle={System Science (HICSS), 2012 45th Hawaii International Conference on},
	pages={3090-3099},
	note={ID: 1},
	abstract={Algorithmic trading has been blamed for an increasing level of volatility in a number of financial markets. Adoption and deployment of algorithmic trading systems has increased and this is likely to continue, as regulation, competition and innovation drive the development of advanced technological tools. Expert and intelligent systems provide the mechanics for both reacting to and affecting a financial market that is now significantly faster and operating across multiple time zones and markets. Surprisingly, much of this innovation has escaped discussion within the Information Systems research community. This paper explores this growing arena by engaging with senior practitioners in the industry and using interviews and grounded theory (GT) analysis to uncover their adoption concerns. The paper generalises these issues within a framework and guidelines aimed at supporting algorithmic trading system adoption, deployment and development.},
	isbn={1530-1605},
	annote={
	?Algorithimc Trading Systems: A multifaceted view of adoption? provided a quick overview of the recent evolutions in how the market works. The market has progressed from being a group of people that meet and agree on how much to trade of their companies for how much to an incredibly quick, complex, and global system that allows anyone to (nearly) instantly trade shares from (nearly) anywhere.  Now it is possible for not only people to do this but also computers. This article presents the experience that IT professionals have had in the adoption of such systems. Topics like: The expense of maintaining a data center that is guaranteed to be up all of the time, being able to scale to exponentially more trades then the system was originally built for, and compliance with federal regulations.\\
This article was only vaguely on the topic I was hoping for, the implementation in software of such systems not the adoption of software in to IT infrastructure. Though it did have a nice little description of some of the evolution of algorithmic trading over the past few years at the beginning.}

}

@inproceedings{RefWorks:18,
	author={G. C. Calafiore and B. Monastero},
	year={2010},
	title={Experiments on stock trading via feedback control},
	booktitle={Information and Financial Engineering (ICIFE), 2010 2nd IEEE International Conference on},
	pages={494-498},
	note={ID: 2},
	abstract={This paper analyses the predictability and return of the Barmish-Iwarere trading algorithm described in. In the first part of the paper, we study the trade triggering algorithm using either an Ito process model, or real data from indexes and ETFs. It is shown through hypothesis testing that the trigger provides mixed results in predicting the sign of the single trade, for both the Ito process and real indexes. However, we show empirically the trigger is sufficiently good in identifying a trend, while it fails in detecting side movements. In the second part of the paper, the effect of parameters of the feedback controller will be analysed under various market circumstances, the efficiency of a pre-optimization on the last data will appear controversal. Some changes will be tried with the objective of improving the returns. In particular, the trigger is modified to detect anomalous falls during a rising trend using the estimated volatility.},
	annote={
	"Experiments on Stock Trading Via Feedback Control". Explores and describes the Barmish-Iwarere (BI) trading algorithim. The paper begins by describing some background information used in BI. First Brownian motion is quickly review as being: a Markov process, having independent increments, and normally distributed over time. Second the Ito process  is described as being a composite of the Wiener process and Brownian motion. The trading system being explored is then described as being composed of a trigger and a controller. The trigger tells the controller when to take and the controller decides how aggressive to take the given action. An Ito process was used to test the system. The trigger takes action if any of the following conditions are true: ?confidence? in the stock is at the lower tolerance level, or the stock is significantly high then the drift or volatility would normally allow for (a market imbalance seems to have been detected). It is indicated that this process is very well optimized but the problem of how to optimize the amount of a risky investment is an open problem. The possibility of using an optimal Kelly fraction (or the Latane strategy) is then explored. The results of the research are then explored with the conclusion that BI is moderately effect and fairly predictable.\\
This article was interesting. It helped me find some more terms (listed below) that may help me in understanding the concepts necessary for effective development, evaluation and discussion of automatic trading systems. The concepts of the Ito process and approximations of the Black-Scholes model seem to be particularly important.
Further research is needed to determine what the following terms are: Wiener process, drift (in the context of stock trading), Brownian motion, optimal Kelly fraction, Latane strategy, Black-Scholes model.
}
}

@inproceedings{RefWorks:23,
	author={S. Hayward},
	year={2004},
	title={Setting up performance surface of an artificial neural network with genetic algorithm optimization: in search of an accurate and profitable prediction of stock trading},
	booktitle={Evolutionary Computation, 2004. CEC2004. Congress on},
	volume={1},
	pages={948-954 Vol.1},
	note={ID: 5},
	abstract={This paper considers a design framework of a computational experiment in finance. The examination of relationships between statistics used for economic forecasts evaluation and profitability of investment decisions reveals that only the 'degree of improvement over efficient prediction' shows robust links with profitability. If profits are not observable, this measure is proposed as an evaluation criterion for an economic prediction. Also combined with directional accuracy, it could be used in an estimation technique for economic behavior, as an alternative to conventional least squares. Model discovery and performance surface optimization with genetic algorithm demonstrate profitability improvement with an inconclusive effect on statistical criteria.},
	annote={
	"Setting Up Performance Surface of an Artificial Neural Network with Genetic Algorithm Optimization: In Search of an Accurate and Profitable Prediction of Stock Trading" talk about various prediction methods used in evolutionary artificial neural network (E/ANN). First the problem is modeled, that being the composition of various (E/ANN) methods and prediction methods to make a decision on whether a trigger should be raised and how much to invest if so. The model and variables that this paper is using to describe the market is then reviewed. Next, the method used to determine the optimal predictor in the context of this paper is reviewed (in this case to use another machine learning algorithm the merits of which are quickly debated against other machine learning methods). The parameters determining the scope of the review of results were defined. The article concluded as not determining any ?best? predictor.
First this article is in a terrible font. The article has some interesting points about how to do analysis on various components of a E/ANN algorithm. While this is not the point of the article it does seem to be something that might be worth duplicating or using as a reference when comparing methods that different researchers used. The articles it cites are also interesting looking I will have to look them up. 
Further research is needed to determine what the following terms are: Surface optimization (in the context of genetic algorithms), autocovarience (which against words belief is a word), Posterior Optimal Rule Signal (PORS), (Backpropagation (another real word) in the context of online machine learning).
}
}

@inproceedings{RefWorks:24,
	author={T. Iokibe and S. Murata and M. Koyama},
	year={1995},
	title={Prediction of foreign exchange rate by local fuzzy reconstruction method},
	booktitle={Systems, Man and Cybernetics, 1995. Intelligent Systems for the 21st Century., IEEE International Conference on},
	volume={5},
	pages={4051-4054 vol.5},
	note={ID: 6},
	abstract={Several systems for the purpose of predicting trends in the foreign exchange market and stocks have been developed. They are either knowledge based expert systems or fuzzy expert systems. The disadvantage},
	annote={
	"Prediction of Foreign Exchange Rate by Local Fuzzy Reconstruction Method" primarily reviews three topics: predicting timeseries data and deterministic chaos, Takens? embedding theorem, local fuzzy reconstruction. Deterministic chaos is defined as being a system that is seemingly chaotic yet is generated by a deterministic source. Takens? embedding theorem is a method of determining the location of a attractor in a chaotic system. A visual example of how this can apply to a two dimensional data source is also presented. Finally the concept of local fuzzy reconstruction is introduced (LFRM). LFRM is a much less expensive way and simpler to calculate with less variables the next probable state in a deterministically chaotic set of behaviors. The article concludes after reviewing a experiment that the system is sufficiently accurate to be used in short term predictions.
As with most things involving chaos (in the mathematical since) attractors are discussed and it seems to me that using strange attractors in the context of predicting the stock market is a remarkably good idea. Also the mention of Takens? theorem is very intriguing and will lead to further research. The idea of remodeling the stock exchange as a multi-dimensional data source also seems like a good idea to me. It makes me wonder if this could be extended to work with longer term predictions or used in concert with other methods to effectively make predictions.
Further research is needed to determine what the following terms are: dynamical, deterministic chaos in a general setting, better understanding of fuzzy logic.
Side note: I don?t care what the world says dynamical IS NOT a word.
	}
}

@inproceedings{RefWorks:21,
	author={G. Kendall and Y. Su},
	year={2004},
	title={Learning with imperfections - a multi-agent neural-genetic trading system with differing levels of social learning},
	booktitle={Cybernetics and Intelligent Systems, 2004 IEEE Conference on},
	volume={1},
	pages={47-52 vol.1},
	note={ID: 3},
	abstract={Some real life dynamic systems are so large and complex that the individuals inside the system can only partially understand their environment. In other words, the dynamic environment is imperfect to its participants. In this paper, by using the stock market as a test bed, we demonstrate an integrated individual learning and social learning model for optimisation problems in dynamic environments with imperfect information. By applying differing levels of social learning process in an evolutionary simulated stock market, we study the importance of social learning on the adaptability of artificial agents in imperfect environments. Comparisons between the integrated individual and social learning model and other evolutionary approaches for dynamic optimisation problems, particularly the memory-based approaches and multipopulation approaches, are also drawn with the emphasis on optimisation problems with imperfect information.},
	annote={
	"Learning with Imperfections - a Multi-Agent Neural-Genetic Trading System with Differing Levels of Social Learning" presents the paradigm of the market being such a complex system that any perceptions that we or computers can make of it are imperfect.  This makes the market an imperfect system (from any useful point of view). The paper also explores how a multi-agent system that communicates with it?s self behaves in this context. First the research is introduced reviewing the components of the research: finding a evolutionary algorithm that not only can find a optimal solution but adapt to the non-static fitness space that is present in the market, and the fact that no matter how much data you provide an agent with it is not possible for them to create a completely accurate predictive model that can be used (imperfect environment) and that this will cause each agent to perceive the environment a unique (possibly useful) way. Next optimization problems (and ideas to overcome them) in dynamic environments are discussed. The primary idea here is that having multiple agents each which evolve to be more effective at smaller problems and then share their knowledge with each other (though not necessarily with the next generation to prevent local optima) may be effective. Then two models of how to do this are discussed. Next the algorithms used in each agent are reviewed, in this case a neural-genetic hybrid algorithm. The rules of the system used to simulate these agents are then described. Following this, how social learning and individual learning work in the context of this experiment is shown in detail. The article concludes with a short description of where further research may continue and infers that this is a very feasible, though imperfect solution.

This article is to dense to summarize all of it?s contents in 300 words? Though I think that the idea of using agents that only evolve a solution to a small subset of the problem is brilliant and needs to be extended to not only just creating different points of view on how the environment works but also to be applied in situations that are carefully selected (by another algorithm) to be a situation that the algorithm will excel in. The idea of them communicating with each other also seems to be very useful and infers that it may be a good idea to have multiple agents looking at any given situation, just like you would have a team of people look at a hard problem. The idea of imperfect environments is one that I think can be applied to many situations because so many real world problems are too complex to accurately model, ways to deal with this may be part of the answer to how to effectively deal with the market.




	}
}

@inproceedings{RefWorks:5,
	author={Xiaohua Wang and P. K. H. Phua and Weidong Lin},
	year={2003},
	title={Stock market prediction using neural networks: Does trading volume help in short-term prediction?},
	booktitle={Neural Networks, 2003. Proceedings of the International Joint Conference on},
	volume={4},
	pages={2438-2442 vol.4},
	note={ID: 3},
	abstract={Recent studies show that there is a significant bidirectional nonlinear causality between stock return and trading volume. This research reinforces the results presented previously and we further investigate whether trading volume can significantly improve the forecasting performance of neural networks, or whether neural networks can adequately model such nonlinearity. Neural networks are trained with the data of stock returns and trading volumes from standard and poor 500 composite index (S&P 500) and Dow Jones Industry index (DJI). The results are used to compare with those networks developed without trading volumes. Daily data is applied to train neural networks in order to test whether trading volumes can help in short-term forecasting. Directional symmetry (DS) and mean absolute percentage error (MAPE) are both employed to test the result of robustness. Empirical results indicate that trading volume has little effect on the performance of direction forecasting. Sometimes it may lead to over-fitting. For forecasting accuracy, trading volume leads to irregular improvements.},
	isbn={1098-7576}
}

@article{RefWorks:19,
	author={J. Grossklags and C. Schmidt},
	year={2006},
	title={Software agents and market (in) efficiency: a human trader experiment},
	journal={Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on},
	volume={36},
	number={1},
	pages={56-67},
	note={ID: 1},
	abstract={This paper studies how software agents influence the market behavior of human traders. Software agents with a passive arbitrage-seeking strategy are introduced in a double auction market experiment with human subjects in the laboratory. As a treatment variable, the influence of information on the existence of software agents is investigated. We found that common knowledge about the presence of software agents triggers more efficient market prices when the programmed strategy was employed, whereas an effect of the information condition on behavioral variables could not be observed. When controlling for information on software agents' participation, the introduction of software agents results in lower market efficiency},
	keywords={electronic trading; marketing data processing; software agents; double auction market; electronic market; human trader; market price; software agent},
	isbn={1094-6977}
}

@inproceedings{RefWorks:7,
	author={M. Kampouridis and Shu-Heng Chen and E. Tsang},
	year={2011},
	title={Investigating the effect of different GP algorithms on the non-stationary behavior of financial markets},
	booktitle={Computational Intelligence for Financial Engineering and Economics (CIFEr), 2011 IEEE Symposium on},
	pages={1-8},
	note={ID: 5},
	abstract={This paper extends a previous market microstructure model, where we used Genetic Programming (GP) as an inference engine for trading rules, and Self Organizing Maps as a clustering machine for those rules. Experiments in that work took place under a single financial market and investigated whether its behavior is non-stationary or cyclic. Results showed that the market's behavior was constantly changing and strategies that would not adapt to these changes, would become obsolete, and their performance would thus decrease over time. However, because experiments in that work were based on a specific GP algorithm, we are interested in this paper to prove that those results are independent of the choice of such algorithms. We thus repeat our previous tests under two more GP frameworks. In addition, while our previous work surveyed only a single market, in this paper we run tests under 10 markets, for generalization purposes. Finally, we deepen our analysis and investigate whether the performance of strategies, which have not co-evolved with the market, follows a continuous decrease, as it has been previously suggested in the agent-based artificial stock market literature. Results show that our previous results are not sensitive to the choice of GP. Strategies that do not co-evolve with the market, become ineffective. However, we do not find evidence for a continuous performance decrease of these strategies.},
	isbn={pending}
}


@article{RefWorks:6,
	author={M. P. Wellman and S. -F Cheng and D. M. Reeves and K. M. Lochner},
	year={2003},
	title={Trading agents competing: performance, progress, and market effectiveness},
	journal={Intelligent Systems, IEEE},
	volume={18},
	number={6},
	pages={48-53},
	note={ID: 4},
	abstract={The annual trading agent competition offers agent designers a forum for evaluating programmed trading techniques in a challenging market scenario. TAC aims to spur research by enabling researchers to compare techniques on a common problem and build on each other's ideas. A fixed set of assumptions and environment settings facilitates communication of methods and results. As a multiyear event, TAC lets researchers observe trading agents' progress over time, in effect accelerating the evolution of an adapted population of traders. Given all the participant effort invested, it is incumbent on us to learn as much from the experience as possible. After three years of TAC, we're ready to examine there we stand. To do this, we used data from actual TAC tournaments and some post-competition experimentation. We based our analysis almost entirely on outcomes (profits and allocations), with very little direct accounting for specific agent techniques.},
	keywords={competitive intelligence; electronic trading; multi-agent systems; software agents; TAC tournament; market effectiveness; multiagent system; trading agent competition},
	isbn={1541-1672}
}

@article{RefWorks:22,
	author={E. W. Saad and D. V. Prokhorov and D. C. Wunsch II},
	year={1998},
	title={Comparative study of stock trend prediction using time delay, recurrent and probabilistic neural networks},
	journal={Neural Networks, IEEE Transactions on},
	volume={9},
	number={6},
	pages={1456-1470},
	note={ID: 4},
	abstract={Three networks are compared for low false alarm stock trend predictions. Short-term trends, particularly attractive for neural network analysis, can be used profitably in scenarios such as option trading, but only with significant risk. Therefore, we focus on limiting false alarms, which improves the risk/reward ratio by preventing losses.},
	keywords={Kalman filters; conjugate gradient methods; feedforward neural nets; filtering theory; forecasting theory; learning (artificial intelligence); multilayer perceptrons; nonlinear filters; recurrent neural nets; stock markets; time series; conjugate gradient training; daily closing price; low false alarm; multistream extended Kalman filter training; option trading; predictability analysis techniques; probabilistic neural networks; recurrent neural networks; risk/reward ratio; short-term trends; stock trend prediction; time delay neural networks},
	isbn={1045-9227}
}










